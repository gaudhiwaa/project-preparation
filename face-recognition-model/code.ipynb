{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nmpl.style.use(\"seaborn-darkgrid\") # style for matplotlib","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-15T05:01:33.803654Z","iopub.execute_input":"2023-04-15T05:01:33.804358Z","iopub.status.idle":"2023-04-15T05:01:43.594477Z","shell.execute_reply.started":"2023-04-15T05:01:33.804320Z","shell.execute_reply":"2023-04-15T05:01:43.593213Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm # loading bar","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:01:43.596903Z","iopub.execute_input":"2023-04-15T05:01:43.598284Z","iopub.status.idle":"2023-04-15T05:01:43.603785Z","shell.execute_reply.started":"2023-04-15T05:01:43.598239Z","shell.execute_reply":"2023-04-15T05:01:43.602522Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"path=\"../input/face-recognition-30/dataset/\"\nfiles=os.listdir(path)\nfiles","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:01:43.605790Z","iopub.execute_input":"2023-04-15T05:01:43.606312Z","iopub.status.idle":"2023-04-15T05:01:43.625887Z","shell.execute_reply.started":"2023-04-15T05:01:43.606272Z","shell.execute_reply":"2023-04-15T05:01:43.624879Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['Courteney_Cox',\n 'arnold_schwarzenegger',\n 'bhuvan_bam',\n 'hardik_pandya',\n 'David_Schwimmer',\n 'Matt_LeBlanc',\n 'Simon_Helberg',\n 'scarlett_johansson',\n 'Pankaj_Tripathi',\n 'Matthew_Perry',\n 'sylvester_stallone',\n 'messi',\n 'Jim_Parsons',\n 'random_person',\n 'Lisa_Kudrow',\n 'mohamed_ali',\n 'brad_pitt',\n 'ronaldo',\n 'virat_kohli',\n 'angelina_jolie',\n 'Kunal_Nayya',\n 'manoj_bajpayee',\n 'Sachin_Tendulka',\n 'Jennifer_Aniston',\n 'dhoni',\n 'pewdiepie',\n 'aishwarya_rai',\n 'Johnny_Galeck',\n 'ROHIT_SHARMA',\n 'suresh_raina']"},"metadata":{}}]},{"cell_type":"code","source":"len(files)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:01:43.629212Z","iopub.execute_input":"2023-04-15T05:01:43.629911Z","iopub.status.idle":"2023-04-15T05:01:43.637166Z","shell.execute_reply.started":"2023-04-15T05:01:43.629868Z","shell.execute_reply":"2023-04-15T05:01:43.635880Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}]},{"cell_type":"code","source":"image_array=[] # a list that will converted to array\nlabel_array=[] # label of file as number\n\n# loop through each sub-folder in train\nfor i in range(len(files)):\n    file_sub=os.listdir(path+files[i])\n    \n    # loop trough files in sub-folder\n    for k in tqdm(range(len(file_sub))):\n        try:\n            img=cv2.imread(path+files[i]+\"/\"+file_sub[k])\n            img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n            img=cv2.resize(img,(96,96))\n            image_array.append(img)\n            label_array.append(i)\n        except:\n            pass","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:01:43.639144Z","iopub.execute_input":"2023-04-15T05:01:43.639582Z","iopub.status.idle":"2023-04-15T05:03:50.963620Z","shell.execute_reply.started":"2023-04-15T05:01:43.639545Z","shell.execute_reply":"2023-04-15T05:03:50.962536Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 702/702 [00:06<00:00, 108.91it/s]\n100%|██████████| 553/553 [00:04<00:00, 126.79it/s]\n100%|██████████| 382/382 [00:02<00:00, 159.46it/s]\n100%|██████████| 292/292 [00:01<00:00, 159.70it/s]\n100%|██████████| 537/537 [00:05<00:00, 106.98it/s]\n100%|██████████| 449/449 [00:03<00:00, 115.04it/s]\n100%|██████████| 484/484 [00:03<00:00, 125.43it/s]\n100%|██████████| 507/507 [00:05<00:00, 96.42it/s] \n100%|██████████| 354/354 [00:02<00:00, 151.63it/s]\n100%|██████████| 530/530 [00:04<00:00, 124.42it/s]\n100%|██████████| 480/480 [00:03<00:00, 122.46it/s]\n100%|██████████| 432/432 [00:03<00:00, 142.08it/s]\n100%|██████████| 639/639 [00:04<00:00, 136.15it/s]\n100%|██████████| 2250/2250 [00:15<00:00, 149.04it/s]\n100%|██████████| 640/640 [00:05<00:00, 112.69it/s]\n100%|██████████| 338/338 [00:02<00:00, 128.56it/s]\n100%|██████████| 552/552 [00:05<00:00, 103.28it/s]\n100%|██████████| 418/418 [00:03<00:00, 138.37it/s]\n100%|██████████| 391/391 [00:03<00:00, 129.05it/s]\n100%|██████████| 465/465 [00:04<00:00, 96.60it/s] \n100%|██████████| 532/532 [00:03<00:00, 145.38it/s]\n100%|██████████| 457/457 [00:02<00:00, 157.65it/s]\n100%|██████████| 354/354 [00:02<00:00, 143.73it/s]\n100%|██████████| 639/639 [00:05<00:00, 108.55it/s]\n100%|██████████| 314/314 [00:01<00:00, 171.41it/s]\n100%|██████████| 395/395 [00:02<00:00, 133.44it/s]\n100%|██████████| 711/711 [00:06<00:00, 116.63it/s]\n100%|██████████| 508/508 [00:04<00:00, 116.43it/s]\n100%|██████████| 265/265 [00:01<00:00, 174.38it/s]\n100%|██████████| 319/319 [00:01<00:00, 183.25it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# look what we have got\nimage_array[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:03:50.965160Z","iopub.execute_input":"2023-04-15T05:03:50.966035Z","iopub.status.idle":"2023-04-15T05:03:50.977256Z","shell.execute_reply.started":"2023-04-15T05:03:50.965995Z","shell.execute_reply":"2023-04-15T05:03:50.976125Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([[[39, 28, 21],\n        [34, 25, 18],\n        [44, 37, 28],\n        ...,\n        [ 8,  0,  1],\n        [ 6,  0,  0],\n        [ 8,  1,  0]],\n\n       [[42, 29, 21],\n        [32, 22, 13],\n        [41, 32, 23],\n        ...,\n        [ 9,  0,  1],\n        [ 7,  1,  1],\n        [ 7,  2,  0]],\n\n       [[34, 21, 13],\n        [40, 30, 21],\n        [53, 45, 34],\n        ...,\n        [ 9,  0,  1],\n        [ 6,  0,  0],\n        [ 5,  0,  0]],\n\n       ...,\n\n       [[11,  4,  1],\n        [16,  7,  3],\n        [25, 16,  9],\n        ...,\n        [28, 21, 15],\n        [37, 29, 26],\n        [37, 27, 25]],\n\n       [[17,  6,  4],\n        [25, 14, 10],\n        [26, 17, 10],\n        ...,\n        [27, 19, 16],\n        [30, 22, 19],\n        [37, 27, 25]],\n\n       [[18,  9,  4],\n        [31, 22, 15],\n        [26, 17, 10],\n        ...,\n        [27, 17, 14],\n        [30, 20, 18],\n        [40, 31, 28]]], dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"# look what we have got\nlabel_array[0:5]","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:03:50.978862Z","iopub.execute_input":"2023-04-15T05:03:50.980018Z","iopub.status.idle":"2023-04-15T05:03:50.987656Z","shell.execute_reply.started":"2023-04-15T05:03:50.979971Z","shell.execute_reply":"2023-04-15T05:03:50.986480Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[0, 0, 0, 0, 0]"},"metadata":{}}]},{"cell_type":"code","source":"import gc\n# free up memory space\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:03:50.989265Z","iopub.execute_input":"2023-04-15T05:03:50.990418Z","iopub.status.idle":"2023-04-15T05:03:51.166732Z","shell.execute_reply.started":"2023-04-15T05:03:50.990382Z","shell.execute_reply":"2023-04-15T05:03:51.165618Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"121"},"metadata":{}}]},{"cell_type":"code","source":"# convert both of them to array\n# divide by 255 to scale image from 0-255 to 0-1\nimage_array=np.array(image_array)/255.0\nlabel_array=np.array(label_array)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:03:51.168584Z","iopub.execute_input":"2023-04-15T05:03:51.168987Z","iopub.status.idle":"2023-04-15T05:03:52.437357Z","shell.execute_reply.started":"2023-04-15T05:03:51.168948Z","shell.execute_reply":"2023-04-15T05:03:52.436308Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"image_array","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:03:52.441494Z","iopub.execute_input":"2023-04-15T05:03:52.441793Z","iopub.status.idle":"2023-04-15T05:03:52.456960Z","shell.execute_reply.started":"2023-04-15T05:03:52.441766Z","shell.execute_reply":"2023-04-15T05:03:52.455689Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([[[[0.15294118, 0.10980392, 0.08235294],\n         [0.13333333, 0.09803922, 0.07058824],\n         [0.17254902, 0.14509804, 0.10980392],\n         ...,\n         [0.03137255, 0.        , 0.00392157],\n         [0.02352941, 0.        , 0.        ],\n         [0.03137255, 0.00392157, 0.        ]],\n\n        [[0.16470588, 0.11372549, 0.08235294],\n         [0.1254902 , 0.08627451, 0.05098039],\n         [0.16078431, 0.1254902 , 0.09019608],\n         ...,\n         [0.03529412, 0.        , 0.00392157],\n         [0.02745098, 0.00392157, 0.00392157],\n         [0.02745098, 0.00784314, 0.        ]],\n\n        [[0.13333333, 0.08235294, 0.05098039],\n         [0.15686275, 0.11764706, 0.08235294],\n         [0.20784314, 0.17647059, 0.13333333],\n         ...,\n         [0.03529412, 0.        , 0.00392157],\n         [0.02352941, 0.        , 0.        ],\n         [0.01960784, 0.        , 0.        ]],\n\n        ...,\n\n        [[0.04313725, 0.01568627, 0.00392157],\n         [0.0627451 , 0.02745098, 0.01176471],\n         [0.09803922, 0.0627451 , 0.03529412],\n         ...,\n         [0.10980392, 0.08235294, 0.05882353],\n         [0.14509804, 0.11372549, 0.10196078],\n         [0.14509804, 0.10588235, 0.09803922]],\n\n        [[0.06666667, 0.02352941, 0.01568627],\n         [0.09803922, 0.05490196, 0.03921569],\n         [0.10196078, 0.06666667, 0.03921569],\n         ...,\n         [0.10588235, 0.0745098 , 0.0627451 ],\n         [0.11764706, 0.08627451, 0.0745098 ],\n         [0.14509804, 0.10588235, 0.09803922]],\n\n        [[0.07058824, 0.03529412, 0.01568627],\n         [0.12156863, 0.08627451, 0.05882353],\n         [0.10196078, 0.06666667, 0.03921569],\n         ...,\n         [0.10588235, 0.06666667, 0.05490196],\n         [0.11764706, 0.07843137, 0.07058824],\n         [0.15686275, 0.12156863, 0.10980392]]],\n\n\n       [[[0.17647059, 0.16862745, 0.17254902],\n         [0.18431373, 0.17647059, 0.18039216],\n         [0.19607843, 0.18823529, 0.19215686],\n         ...,\n         [0.81176471, 0.81960784, 0.83529412],\n         [0.80784314, 0.82352941, 0.83921569],\n         [0.80784314, 0.82745098, 0.85098039]],\n\n        [[0.18039216, 0.17254902, 0.17647059],\n         [0.18431373, 0.17647059, 0.18039216],\n         [0.19215686, 0.18431373, 0.18823529],\n         ...,\n         [0.81960784, 0.82745098, 0.83921569],\n         [0.81176471, 0.82745098, 0.84313725],\n         [0.81176471, 0.82745098, 0.84705882]],\n\n        [[0.18039216, 0.17647059, 0.17254902],\n         [0.18823529, 0.18039216, 0.17647059],\n         [0.18431373, 0.18039216, 0.17647059],\n         ...,\n         [0.81960784, 0.82745098, 0.83921569],\n         [0.81568627, 0.82745098, 0.84313725],\n         [0.81176471, 0.82352941, 0.84313725]],\n\n        ...,\n\n        [[0.22352941, 0.21176471, 0.21568627],\n         [0.2       , 0.19215686, 0.19607843],\n         [0.15686275, 0.15294118, 0.15294118],\n         ...,\n         [0.15294118, 0.15294118, 0.15294118],\n         [0.15686275, 0.15686275, 0.15294118],\n         [0.16470588, 0.16470588, 0.15294118]],\n\n        [[0.21960784, 0.21176471, 0.21568627],\n         [0.2       , 0.19215686, 0.19607843],\n         [0.15686275, 0.15294118, 0.15294118],\n         ...,\n         [0.15686275, 0.15686275, 0.14901961],\n         [0.15294118, 0.15294118, 0.14117647],\n         [0.15686275, 0.16078431, 0.14117647]],\n\n        [[0.21568627, 0.21568627, 0.21568627],\n         [0.19607843, 0.19607843, 0.19607843],\n         [0.15686275, 0.15686275, 0.15686275],\n         ...,\n         [0.15686275, 0.15686275, 0.14901961],\n         [0.14901961, 0.14901961, 0.14117647],\n         [0.14901961, 0.15686275, 0.1372549 ]]],\n\n\n       [[[0.03529412, 0.05490196, 0.02745098],\n         [0.03529412, 0.05490196, 0.03137255],\n         [0.03137255, 0.05098039, 0.02745098],\n         ...,\n         [0.02352941, 0.02352941, 0.03137255],\n         [0.01960784, 0.01960784, 0.02352941],\n         [0.01960784, 0.01960784, 0.01960784]],\n\n        [[0.02745098, 0.04705882, 0.02352941],\n         [0.02745098, 0.04705882, 0.02352941],\n         [0.03137255, 0.05098039, 0.02745098],\n         ...,\n         [0.01960784, 0.01960784, 0.01960784],\n         [0.01960784, 0.01960784, 0.01960784],\n         [0.01960784, 0.01960784, 0.01960784]],\n\n        [[0.01568627, 0.03529412, 0.01176471],\n         [0.01568627, 0.03529412, 0.01176471],\n         [0.01176471, 0.03137255, 0.00784314],\n         ...,\n         [0.01960784, 0.01960784, 0.01176471],\n         [0.01960784, 0.01960784, 0.01176471],\n         [0.01960784, 0.01960784, 0.01176471]],\n\n        ...,\n\n        [[0.20392157, 0.18039216, 0.19215686],\n         [0.18039216, 0.16470588, 0.16862745],\n         [0.11372549, 0.10588235, 0.10980392],\n         ...,\n         [0.01960784, 0.01568627, 0.03529412],\n         [0.01960784, 0.01568627, 0.03529412],\n         [0.01960784, 0.01568627, 0.03529412]],\n\n        [[0.15686275, 0.13333333, 0.14117647],\n         [0.21960784, 0.20392157, 0.21176471],\n         [0.09803922, 0.07843137, 0.09411765],\n         ...,\n         [0.01960784, 0.01568627, 0.03529412],\n         [0.01960784, 0.01568627, 0.03529412],\n         [0.01960784, 0.01568627, 0.03529412]],\n\n        [[0.1372549 , 0.11372549, 0.12156863],\n         [0.14509804, 0.12941176, 0.13333333],\n         [0.17647059, 0.15686275, 0.17254902],\n         ...,\n         [0.01960784, 0.01568627, 0.03529412],\n         [0.01960784, 0.01568627, 0.03529412],\n         [0.01960784, 0.01568627, 0.03529412]]],\n\n\n       ...,\n\n\n       [[[0.10588235, 0.11372549, 0.09019608],\n         [0.10980392, 0.11764706, 0.10196078],\n         [0.04705882, 0.04705882, 0.03921569],\n         ...,\n         [0.4       , 0.45098039, 0.4627451 ],\n         [0.78039216, 0.81176471, 0.81960784],\n         [0.90196078, 0.9254902 , 0.93333333]],\n\n        [[0.14117647, 0.14901961, 0.1254902 ],\n         [0.11372549, 0.11372549, 0.10196078],\n         [0.06666667, 0.0627451 , 0.05882353],\n         ...,\n         [0.34509804, 0.36078431, 0.37254902],\n         [0.69411765, 0.70196078, 0.71372549],\n         [0.91764706, 0.9254902 , 0.92941176]],\n\n        [[0.1254902 , 0.12941176, 0.10980392],\n         [0.13333333, 0.13333333, 0.11764706],\n         [0.10980392, 0.10588235, 0.09803922],\n         ...,\n         [0.93333333, 0.93333333, 0.94117647],\n         [0.92941176, 0.92941176, 0.93333333],\n         [0.9254902 , 0.9254902 , 0.91764706]],\n\n        ...,\n\n        [[0.99607843, 0.99607843, 0.98823529],\n         [0.94117647, 1.        , 1.        ],\n         [0.41568627, 0.63529412, 0.77647059],\n         ...,\n         [1.        , 1.        , 1.        ],\n         [1.        , 1.        , 1.        ],\n         [1.        , 1.        , 1.        ]],\n\n        [[0.99215686, 0.99607843, 0.98039216],\n         [0.9372549 , 1.        , 1.        ],\n         [0.38431373, 0.61176471, 0.76078431],\n         ...,\n         [1.        , 1.        , 1.        ],\n         [1.        , 1.        , 1.        ],\n         [1.        , 1.        , 1.        ]],\n\n        [[0.99215686, 1.        , 0.99215686],\n         [0.94117647, 1.        , 1.        ],\n         [0.41568627, 0.62745098, 0.77254902],\n         ...,\n         [1.        , 1.        , 1.        ],\n         [1.        , 1.        , 1.        ],\n         [1.        , 1.        , 1.        ]]],\n\n\n       [[[0.85490196, 0.81176471, 0.73333333],\n         [0.8745098 , 0.83529412, 0.75294118],\n         [0.74509804, 0.70196078, 0.63529412],\n         ...,\n         [0.25098039, 0.26666667, 0.40392157],\n         [0.21960784, 0.25490196, 0.37254902],\n         [0.04313725, 0.09019608, 0.18431373]],\n\n        [[0.8745098 , 0.81568627, 0.74901961],\n         [0.85098039, 0.78823529, 0.74509804],\n         [0.74117647, 0.67843137, 0.64313725],\n         ...,\n         [0.18431373, 0.20784314, 0.33333333],\n         [0.41960784, 0.45490196, 0.56078431],\n         [0.29803922, 0.34117647, 0.43137255]],\n\n        [[0.85098039, 0.78431373, 0.72156863],\n         [0.80784314, 0.72941176, 0.72156863],\n         [0.67058824, 0.6       , 0.6       ],\n         ...,\n         [0.49411765, 0.5254902 , 0.63921569],\n         [0.63921569, 0.67843137, 0.76470588],\n         [0.59607843, 0.62745098, 0.70980392]],\n\n        ...,\n\n        [[0.81960784, 0.82352941, 0.8       ],\n         [0.81960784, 0.82352941, 0.8       ],\n         [0.81960784, 0.82352941, 0.79215686],\n         ...,\n         [0.83529412, 0.69019608, 0.24705882],\n         [0.8       , 0.67843137, 0.15686275],\n         [0.77647059, 0.65882353, 0.17647059]],\n\n        [[0.80784314, 0.82352941, 0.79607843],\n         [0.81568627, 0.82352941, 0.8       ],\n         [0.82352941, 0.83137255, 0.8       ],\n         ...,\n         [0.82352941, 0.68235294, 0.25490196],\n         [0.80784314, 0.68235294, 0.18823529],\n         [0.78823529, 0.66666667, 0.21176471]],\n\n        [[0.80784314, 0.82745098, 0.8       ],\n         [0.80784314, 0.82745098, 0.8       ],\n         [0.80784314, 0.82745098, 0.8       ],\n         ...,\n         [0.85490196, 0.69019608, 0.28627451],\n         [0.78823529, 0.64705882, 0.17254902],\n         [0.77254902, 0.62352941, 0.18823529]]],\n\n\n       [[[0.61176471, 0.36470588, 0.        ],\n         [0.61176471, 0.36470588, 0.00784314],\n         [0.61176471, 0.36078431, 0.01176471],\n         ...,\n         [0.98039216, 0.87843137, 0.12941176],\n         [0.98431373, 0.8745098 , 0.1372549 ],\n         [0.98431373, 0.8745098 , 0.1372549 ]],\n\n        [[0.60784314, 0.35686275, 0.00392157],\n         [0.60784314, 0.36078431, 0.00784314],\n         [0.60784314, 0.36078431, 0.01176471],\n         ...,\n         [0.98431373, 0.87843137, 0.1372549 ],\n         [0.98823529, 0.87843137, 0.14117647],\n         [0.99215686, 0.88235294, 0.14901961]],\n\n        [[0.60392157, 0.35686275, 0.00784314],\n         [0.6       , 0.35686275, 0.01176471],\n         [0.6       , 0.35686275, 0.01568627],\n         ...,\n         [0.96862745, 0.86666667, 0.13333333],\n         [0.97647059, 0.87058824, 0.1372549 ],\n         [0.98431373, 0.87843137, 0.14901961]],\n\n        ...,\n\n        [[0.05098039, 0.07058824, 0.04705882],\n         [0.05098039, 0.07058824, 0.04705882],\n         [0.05098039, 0.07058824, 0.04705882],\n         ...,\n         [0.18823529, 0.11764706, 0.05098039],\n         [0.18039216, 0.10980392, 0.03921569],\n         [0.16862745, 0.09411765, 0.01960784]],\n\n        [[0.05098039, 0.07058824, 0.04705882],\n         [0.05098039, 0.07058824, 0.04705882],\n         [0.05098039, 0.07058824, 0.04705882],\n         ...,\n         [0.18039216, 0.10980392, 0.05490196],\n         [0.17647059, 0.10196078, 0.04313725],\n         [0.16078431, 0.08627451, 0.02352941]],\n\n        [[0.05098039, 0.07058824, 0.04705882],\n         [0.05098039, 0.07058824, 0.04705882],\n         [0.05098039, 0.07058824, 0.04705882],\n         ...,\n         [0.17254902, 0.10196078, 0.05098039],\n         [0.16862745, 0.09411765, 0.03921569],\n         [0.15686275, 0.08235294, 0.02352941]]]])"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# split datatest to : train (85%) and test (15%)\nX_train,X_test,Y_train,Y_test=train_test_split(image_array,label_array,test_size=0.15)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:03:52.458518Z","iopub.execute_input":"2023-04-15T05:03:52.458953Z","iopub.status.idle":"2023-04-15T05:03:53.761585Z","shell.execute_reply.started":"2023-04-15T05:03:52.458917Z","shell.execute_reply":"2023-04-15T05:03:53.760482Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# create the model using classification\nfrom keras import layers,callbacks,utils,applications,optimizers\nfrom keras.models import Sequential,Model,load_model","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:03:53.763286Z","iopub.execute_input":"2023-04-15T05:03:53.763705Z","iopub.status.idle":"2023-04-15T05:03:53.769489Z","shell.execute_reply.started":"2023-04-15T05:03:53.763661Z","shell.execute_reply":"2023-04-15T05:03:53.768250Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model=Sequential()\n# use MobileNetV2 as an pretrained model \npretrained_model=tf.keras.applications.EfficientNetB0(input_shape=(96,96,3),include_top=False,weights=\"imagenet\")\nmodel.add(pretrained_model)\nmodel.add(layers.GlobalAveragePooling2D())\n# add dropout to increase accuracy by not overfitting\nmodel.add(layers.Dropout(0.3))\n# add dense layer as final output\nmodel.add(layers.Dense(1))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:03:53.771146Z","iopub.execute_input":"2023-04-15T05:03:53.771565Z","iopub.status.idle":"2023-04-15T05:04:00.619211Z","shell.execute_reply.started":"2023-04-15T05:03:53.771528Z","shell.execute_reply":"2023-04-15T05:04:00.618125Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n16705208/16705208 [==============================] - 0s 0us/step\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n efficientnetb0 (Functional)  (None, 3, 3, 1280)       4049571   \n                                                                 \n global_average_pooling2d (G  (None, 1280)             0         \n lobalAveragePooling2D)                                          \n                                                                 \n dropout (Dropout)           (None, 1280)              0         \n                                                                 \n dense (Dense)               (None, 1)                 1281      \n                                                                 \n=================================================================\nTotal params: 4,050,852\nTrainable params: 4,008,829\nNon-trainable params: 42,023\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# compile the model\nmodel.compile(optimizer=\"adam\",loss=\"mean_squared_error\",metrics=[\"mae\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:04:00.620606Z","iopub.execute_input":"2023-04-15T05:04:00.621221Z","iopub.status.idle":"2023-04-15T05:04:00.644151Z","shell.execute_reply.started":"2023-04-15T05:04:00.621182Z","shell.execute_reply":"2023-04-15T05:04:00.643222Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# creating a chechpoint to save model at best accuarcy\n\nckp_path=\"trained_model/model\"\nmodel_checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath=ckp_path,\n                                                   monitor=\"val_mae\",\n                                                   mode=\"auto\",\n                                                   save_best_only=True,\n                                                   save_weights_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:04:00.645381Z","iopub.execute_input":"2023-04-15T05:04:00.646222Z","iopub.status.idle":"2023-04-15T05:04:00.652039Z","shell.execute_reply.started":"2023-04-15T05:04:00.646185Z","shell.execute_reply":"2023-04-15T05:04:00.650900Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# create a lr reducer which decrease learning rate when accuarcy does not increase\nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau(factor=0.9,monitor=\"val_mae\",\n                                             mode=\"auto\",cooldown=0,\n                                             patience=5,verbose=1,min_lr=1e-6)\n# patience : wait till 5 epoch\n# verbose : show accuracy every 1 epoch\n# min_lr=minimum learning rate","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:04:00.653452Z","iopub.execute_input":"2023-04-15T05:04:00.653989Z","iopub.status.idle":"2023-04-15T05:04:00.661886Z","shell.execute_reply.started":"2023-04-15T05:04:00.653953Z","shell.execute_reply":"2023-04-15T05:04:00.661029Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"EPOCHS=250\nBATCH_SIZE=64\n\nhistory=model.fit(X_train,\n                 Y_train,\n                 validation_data=(X_test,Y_test),\n                 batch_size=BATCH_SIZE,\n                 epochs=EPOCHS,\n                 callbacks=[model_checkpoint,reduce_lr]\n                 )\n","metadata":{"execution":{"iopub.status.busy":"2023-04-15T05:04:00.663168Z","iopub.execute_input":"2023-04-15T05:04:00.663888Z","iopub.status.idle":"2023-04-15T06:39:45.707487Z","shell.execute_reply.started":"2023-04-15T05:04:00.663852Z","shell.execute_reply":"2023-04-15T06:39:45.700547Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/250\n","output_type":"stream"},{"name":"stderr","text":"2023-04-15 05:04:21.666208: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/efficientnetb0/block2b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"212/212 [==============================] - 75s 132ms/step - loss: 48.6125 - mae: 5.2234 - val_loss: 130.9044 - val_mae: 9.3121 - lr: 0.0010\nEpoch 2/250\n212/212 [==============================] - 23s 110ms/step - loss: 19.5191 - mae: 3.1901 - val_loss: 84.5668 - val_mae: 7.4695 - lr: 0.0010\nEpoch 3/250\n212/212 [==============================] - 23s 110ms/step - loss: 13.0086 - mae: 2.5075 - val_loss: 76.5414 - val_mae: 7.1062 - lr: 0.0010\nEpoch 4/250\n212/212 [==============================] - 23s 110ms/step - loss: 9.9551 - mae: 2.1557 - val_loss: 43.4295 - val_mae: 5.1040 - lr: 0.0010\nEpoch 5/250\n212/212 [==============================] - 23s 108ms/step - loss: 8.1676 - mae: 1.9132 - val_loss: 77.4033 - val_mae: 7.1351 - lr: 0.0010\nEpoch 6/250\n212/212 [==============================] - 23s 106ms/step - loss: 7.0835 - mae: 1.7596 - val_loss: 68.3748 - val_mae: 6.6791 - lr: 0.0010\nEpoch 7/250\n212/212 [==============================] - 23s 107ms/step - loss: 5.1124 - mae: 1.4896 - val_loss: 76.2070 - val_mae: 7.0863 - lr: 0.0010\nEpoch 8/250\n212/212 [==============================] - 23s 107ms/step - loss: 6.7283 - mae: 1.6802 - val_loss: 90.5070 - val_mae: 7.6177 - lr: 0.0010\nEpoch 9/250\n211/212 [============================>.] - ETA: 0s - loss: 5.3642 - mae: 1.4542\nEpoch 9: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n212/212 [==============================] - 23s 108ms/step - loss: 5.3643 - mae: 1.4542 - val_loss: 79.9289 - val_mae: 7.1196 - lr: 0.0010\nEpoch 10/250\n212/212 [==============================] - 22s 106ms/step - loss: 3.6035 - mae: 1.2058 - val_loss: 61.0667 - val_mae: 6.2674 - lr: 9.0000e-04\nEpoch 11/250\n212/212 [==============================] - 22s 106ms/step - loss: 3.2627 - mae: 1.1564 - val_loss: 212.4990 - val_mae: 12.2755 - lr: 9.0000e-04\nEpoch 12/250\n212/212 [==============================] - 23s 108ms/step - loss: 3.2499 - mae: 1.1307 - val_loss: 64.6505 - val_mae: 6.4388 - lr: 9.0000e-04\nEpoch 13/250\n212/212 [==============================] - 23s 107ms/step - loss: 2.9165 - mae: 1.0503 - val_loss: 60.5226 - val_mae: 6.1499 - lr: 9.0000e-04\nEpoch 14/250\n211/212 [============================>.] - ETA: 0s - loss: 5.0850 - mae: 1.3733\nEpoch 14: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n212/212 [==============================] - 23s 106ms/step - loss: 5.0863 - mae: 1.3736 - val_loss: 66.7152 - val_mae: 6.5663 - lr: 9.0000e-04\nEpoch 15/250\n212/212 [==============================] - 23s 107ms/step - loss: 3.5544 - mae: 1.1417 - val_loss: 64.8516 - val_mae: 6.4976 - lr: 8.1000e-04\nEpoch 16/250\n212/212 [==============================] - 22s 106ms/step - loss: 2.7649 - mae: 1.0030 - val_loss: 67.1251 - val_mae: 6.6134 - lr: 8.1000e-04\nEpoch 17/250\n212/212 [==============================] - 23s 106ms/step - loss: 2.4305 - mae: 0.9716 - val_loss: 66.9029 - val_mae: 6.5885 - lr: 8.1000e-04\nEpoch 18/250\n212/212 [==============================] - 22s 106ms/step - loss: 2.1576 - mae: 0.8892 - val_loss: 72.5227 - val_mae: 6.8941 - lr: 8.1000e-04\nEpoch 19/250\n211/212 [============================>.] - ETA: 0s - loss: 3.9719 - mae: 1.2279\nEpoch 19: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n212/212 [==============================] - 23s 108ms/step - loss: 3.9844 - mae: 1.2288 - val_loss: 66.2298 - val_mae: 6.7256 - lr: 8.1000e-04\nEpoch 20/250\n212/212 [==============================] - 22s 106ms/step - loss: 3.1831 - mae: 1.0816 - val_loss: 67.1984 - val_mae: 6.5862 - lr: 7.2900e-04\nEpoch 21/250\n212/212 [==============================] - 23s 107ms/step - loss: 1.4168 - mae: 0.7556 - val_loss: 59.4677 - val_mae: 6.1659 - lr: 7.2900e-04\nEpoch 22/250\n212/212 [==============================] - 23s 107ms/step - loss: 3.3297 - mae: 1.0810 - val_loss: 71.5808 - val_mae: 6.8371 - lr: 7.2900e-04\nEpoch 23/250\n212/212 [==============================] - 22s 105ms/step - loss: 1.6265 - mae: 0.7881 - val_loss: 161.1851 - val_mae: 10.4574 - lr: 7.2900e-04\nEpoch 24/250\n211/212 [============================>.] - ETA: 0s - loss: 1.5663 - mae: 0.7398\nEpoch 24: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n212/212 [==============================] - 23s 107ms/step - loss: 1.5689 - mae: 0.7402 - val_loss: 85.0533 - val_mae: 7.6021 - lr: 7.2900e-04\nEpoch 25/250\n212/212 [==============================] - 22s 106ms/step - loss: 1.4265 - mae: 0.7273 - val_loss: 65.1926 - val_mae: 6.5945 - lr: 6.5610e-04\nEpoch 26/250\n212/212 [==============================] - 22s 106ms/step - loss: 1.0780 - mae: 0.6461 - val_loss: 48.8562 - val_mae: 5.1913 - lr: 6.5610e-04\nEpoch 27/250\n212/212 [==============================] - 22s 105ms/step - loss: 1.1332 - mae: 0.6944 - val_loss: 65.1409 - val_mae: 6.4480 - lr: 6.5610e-04\nEpoch 28/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.9398 - mae: 0.5892 - val_loss: 65.5153 - val_mae: 6.5522 - lr: 6.5610e-04\nEpoch 29/250\n211/212 [============================>.] - ETA: 0s - loss: 2.6029 - mae: 0.8963\nEpoch 29: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n212/212 [==============================] - 22s 106ms/step - loss: 2.6095 - mae: 0.8970 - val_loss: 71.1307 - val_mae: 6.7998 - lr: 6.5610e-04\nEpoch 30/250\n212/212 [==============================] - 22s 105ms/step - loss: 1.9785 - mae: 0.8125 - val_loss: 53.5955 - val_mae: 5.8031 - lr: 5.9049e-04\nEpoch 31/250\n212/212 [==============================] - 22s 106ms/step - loss: 2.7021 - mae: 0.9182 - val_loss: 64.8977 - val_mae: 6.4994 - lr: 5.9049e-04\nEpoch 32/250\n212/212 [==============================] - 22s 106ms/step - loss: 2.3485 - mae: 0.8703 - val_loss: 66.7755 - val_mae: 6.5886 - lr: 5.9049e-04\nEpoch 33/250\n212/212 [==============================] - 28s 134ms/step - loss: 1.5785 - mae: 0.7514 - val_loss: 42.8505 - val_mae: 4.8706 - lr: 5.9049e-04\nEpoch 34/250\n212/212 [==============================] - 23s 106ms/step - loss: 1.1018 - mae: 0.6507 - val_loss: 64.4023 - val_mae: 6.4378 - lr: 5.9049e-04\nEpoch 35/250\n212/212 [==============================] - 23s 106ms/step - loss: 1.5323 - mae: 0.7212 - val_loss: 62.7035 - val_mae: 6.4700 - lr: 5.9049e-04\nEpoch 36/250\n212/212 [==============================] - 22s 106ms/step - loss: 1.6835 - mae: 0.7739 - val_loss: 59.0212 - val_mae: 6.1175 - lr: 5.9049e-04\nEpoch 37/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.9767 - mae: 0.6112 - val_loss: 51.4114 - val_mae: 5.5599 - lr: 5.9049e-04\nEpoch 38/250\n212/212 [==============================] - 23s 110ms/step - loss: 0.8403 - mae: 0.5751 - val_loss: 36.1997 - val_mae: 4.3017 - lr: 5.9049e-04\nEpoch 39/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.7486 - mae: 0.5595 - val_loss: 62.0737 - val_mae: 6.3611 - lr: 5.9049e-04\nEpoch 40/250\n212/212 [==============================] - 23s 106ms/step - loss: 1.2300 - mae: 0.6098 - val_loss: 59.9370 - val_mae: 6.1977 - lr: 5.9049e-04\nEpoch 41/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.8211 - mae: 0.5550 - val_loss: 63.8826 - val_mae: 6.3034 - lr: 5.9049e-04\nEpoch 42/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.6698 - mae: 0.5241 - val_loss: 47.6079 - val_mae: 5.2280 - lr: 5.9049e-04\nEpoch 43/250\n211/212 [============================>.] - ETA: 0s - loss: 1.0250 - mae: 0.5771\nEpoch 43: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n212/212 [==============================] - 22s 105ms/step - loss: 1.0260 - mae: 0.5774 - val_loss: 70.2971 - val_mae: 6.8031 - lr: 5.9049e-04\nEpoch 44/250\n212/212 [==============================] - 22s 106ms/step - loss: 1.6733 - mae: 0.7099 - val_loss: 47.5630 - val_mae: 5.3244 - lr: 5.3144e-04\nEpoch 45/250\n212/212 [==============================] - 23s 106ms/step - loss: 1.0800 - mae: 0.5933 - val_loss: 61.1017 - val_mae: 6.1954 - lr: 5.3144e-04\nEpoch 46/250\n212/212 [==============================] - 22s 106ms/step - loss: 1.0206 - mae: 0.5941 - val_loss: 66.1724 - val_mae: 6.5426 - lr: 5.3144e-04\nEpoch 47/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.6830 - mae: 0.5335 - val_loss: 58.1431 - val_mae: 6.1074 - lr: 5.3144e-04\nEpoch 48/250\n211/212 [============================>.] - ETA: 0s - loss: 1.7814 - mae: 0.7468\nEpoch 48: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n212/212 [==============================] - 22s 105ms/step - loss: 1.7813 - mae: 0.7468 - val_loss: 203.2421 - val_mae: 11.9787 - lr: 5.3144e-04\nEpoch 49/250\n212/212 [==============================] - 23s 110ms/step - loss: 0.6577 - mae: 0.5148 - val_loss: 13.7665 - val_mae: 1.7769 - lr: 4.7830e-04\nEpoch 50/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.9522 - mae: 0.5459 - val_loss: 64.4670 - val_mae: 6.5443 - lr: 4.7830e-04\nEpoch 51/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.7561 - mae: 0.5203 - val_loss: 55.3473 - val_mae: 5.7471 - lr: 4.7830e-04\nEpoch 52/250\n212/212 [==============================] - 23s 107ms/step - loss: 1.2886 - mae: 0.6558 - val_loss: 62.8174 - val_mae: 6.4860 - lr: 4.7830e-04\nEpoch 53/250\n212/212 [==============================] - 22s 106ms/step - loss: 1.8104 - mae: 0.7262 - val_loss: 58.2842 - val_mae: 6.0635 - lr: 4.7830e-04\nEpoch 54/250\n211/212 [============================>.] - ETA: 0s - loss: 0.6096 - mae: 0.5023\nEpoch 54: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n212/212 [==============================] - 23s 107ms/step - loss: 0.6128 - mae: 0.5028 - val_loss: 61.6647 - val_mae: 6.2822 - lr: 4.7830e-04\nEpoch 55/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.6089 - mae: 0.4909 - val_loss: 66.3454 - val_mae: 6.5643 - lr: 4.3047e-04\nEpoch 56/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.9788 - mae: 0.5812 - val_loss: 77.9271 - val_mae: 7.2617 - lr: 4.3047e-04\nEpoch 57/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.7889 - mae: 0.5014 - val_loss: 53.6294 - val_mae: 5.6875 - lr: 4.3047e-04\nEpoch 58/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.6044 - mae: 0.4880 - val_loss: 63.8702 - val_mae: 6.4258 - lr: 4.3047e-04\nEpoch 59/250\n211/212 [============================>.] - ETA: 0s - loss: 0.7523 - mae: 0.5232\nEpoch 59: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n212/212 [==============================] - 22s 105ms/step - loss: 0.7523 - mae: 0.5232 - val_loss: 59.8877 - val_mae: 6.1056 - lr: 4.3047e-04\nEpoch 60/250\n212/212 [==============================] - 23s 108ms/step - loss: 0.6065 - mae: 0.4693 - val_loss: 62.9087 - val_mae: 6.3680 - lr: 3.8742e-04\nEpoch 61/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.4593 - mae: 0.4500 - val_loss: 26.5495 - val_mae: 3.0644 - lr: 3.8742e-04\nEpoch 62/250\n212/212 [==============================] - 23s 107ms/step - loss: 1.0655 - mae: 0.5895 - val_loss: 66.4943 - val_mae: 6.5597 - lr: 3.8742e-04\nEpoch 63/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.5249 - mae: 0.4682 - val_loss: 58.8308 - val_mae: 6.0006 - lr: 3.8742e-04\nEpoch 64/250\n211/212 [============================>.] - ETA: 0s - loss: 0.4606 - mae: 0.4552\nEpoch 64: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n212/212 [==============================] - 23s 107ms/step - loss: 0.4725 - mae: 0.4561 - val_loss: 48.5865 - val_mae: 4.9340 - lr: 3.8742e-04\nEpoch 65/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.6170 - mae: 0.4841 - val_loss: 63.7672 - val_mae: 6.3736 - lr: 3.4868e-04\nEpoch 66/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.5906 - mae: 0.5068 - val_loss: 22.6187 - val_mae: 2.6999 - lr: 3.4868e-04\nEpoch 67/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.4180 - mae: 0.4374 - val_loss: 16.9517 - val_mae: 2.0776 - lr: 3.4868e-04\nEpoch 68/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.4683 - mae: 0.4443 - val_loss: 63.9145 - val_mae: 6.4071 - lr: 3.4868e-04\nEpoch 69/250\n211/212 [============================>.] - ETA: 0s - loss: 0.4981 - mae: 0.4364\nEpoch 69: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n212/212 [==============================] - 22s 105ms/step - loss: 0.4999 - mae: 0.4367 - val_loss: 30.4255 - val_mae: 3.6179 - lr: 3.4868e-04\nEpoch 70/250\n212/212 [==============================] - 23s 108ms/step - loss: 0.3993 - mae: 0.4215 - val_loss: 20.9696 - val_mae: 2.6653 - lr: 3.1381e-04\nEpoch 71/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.4134 - mae: 0.4125 - val_loss: 27.0050 - val_mae: 3.2186 - lr: 3.1381e-04\nEpoch 72/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.8863 - mae: 0.5224 - val_loss: 66.1501 - val_mae: 6.5755 - lr: 3.1381e-04\nEpoch 73/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.5523 - mae: 0.4592 - val_loss: 36.5489 - val_mae: 4.1680 - lr: 3.1381e-04\nEpoch 74/250\n211/212 [============================>.] - ETA: 0s - loss: 0.7519 - mae: 0.5355\nEpoch 74: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n212/212 [==============================] - 23s 107ms/step - loss: 0.7585 - mae: 0.5362 - val_loss: 58.3840 - val_mae: 5.9417 - lr: 3.1381e-04\nEpoch 75/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.6807 - mae: 0.4774 - val_loss: 50.7752 - val_mae: 5.3683 - lr: 2.8243e-04\nEpoch 76/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.4171 - mae: 0.4227 - val_loss: 44.2814 - val_mae: 4.7093 - lr: 2.8243e-04\nEpoch 77/250\n212/212 [==============================] - 24s 112ms/step - loss: 0.3709 - mae: 0.4116 - val_loss: 10.8883 - val_mae: 1.3977 - lr: 2.8243e-04\nEpoch 78/250\n212/212 [==============================] - 24s 111ms/step - loss: 0.3753 - mae: 0.4086 - val_loss: 8.5802 - val_mae: 1.1540 - lr: 2.8243e-04\nEpoch 79/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.3013 - mae: 0.3828 - val_loss: 25.8515 - val_mae: 2.9686 - lr: 2.8243e-04\nEpoch 80/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.4179 - mae: 0.4261 - val_loss: 36.0802 - val_mae: 3.8396 - lr: 2.8243e-04\nEpoch 81/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.4346 - mae: 0.4252 - val_loss: 47.8794 - val_mae: 5.1994 - lr: 2.8243e-04\nEpoch 82/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.7594 - mae: 0.5052 - val_loss: 66.1569 - val_mae: 6.6268 - lr: 2.8243e-04\nEpoch 83/250\n211/212 [============================>.] - ETA: 0s - loss: 0.4861 - mae: 0.4238\nEpoch 83: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n212/212 [==============================] - 23s 107ms/step - loss: 0.4861 - mae: 0.4238 - val_loss: 26.5346 - val_mae: 2.9522 - lr: 2.8243e-04\nEpoch 84/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.3265 - mae: 0.3901 - val_loss: 20.8272 - val_mae: 2.6082 - lr: 2.5419e-04\nEpoch 85/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.3432 - mae: 0.3834 - val_loss: 8.7613 - val_mae: 1.2007 - lr: 2.5419e-04\nEpoch 86/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.2956 - mae: 0.3790 - val_loss: 10.6851 - val_mae: 1.4175 - lr: 2.5419e-04\nEpoch 87/250\n212/212 [==============================] - 24s 113ms/step - loss: 0.2653 - mae: 0.3675 - val_loss: 7.8688 - val_mae: 0.9901 - lr: 2.5419e-04\nEpoch 88/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2627 - mae: 0.3582 - val_loss: 9.1156 - val_mae: 1.1228 - lr: 2.5419e-04\nEpoch 89/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.2762 - mae: 0.3587 - val_loss: 17.9184 - val_mae: 2.1039 - lr: 2.5419e-04\nEpoch 90/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.2667 - mae: 0.3567 - val_loss: 47.7993 - val_mae: 4.8792 - lr: 2.5419e-04\nEpoch 91/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.3050 - mae: 0.3786 - val_loss: 34.9961 - val_mae: 3.8790 - lr: 2.5419e-04\nEpoch 92/250\n212/212 [==============================] - ETA: 0s - loss: 0.2837 - mae: 0.3626\nEpoch 92: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n212/212 [==============================] - 22s 106ms/step - loss: 0.2837 - mae: 0.3626 - val_loss: 20.9433 - val_mae: 2.4412 - lr: 2.5419e-04\nEpoch 93/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2744 - mae: 0.3664 - val_loss: 59.9310 - val_mae: 6.0561 - lr: 2.2877e-04\nEpoch 94/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.3491 - mae: 0.3911 - val_loss: 34.8810 - val_mae: 3.9821 - lr: 2.2877e-04\nEpoch 95/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.3136 - mae: 0.3644 - val_loss: 25.9082 - val_mae: 2.9058 - lr: 2.2877e-04\nEpoch 96/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.2834 - mae: 0.3639 - val_loss: 55.6538 - val_mae: 5.8918 - lr: 2.2877e-04\nEpoch 97/250\n211/212 [============================>.] - ETA: 0s - loss: 0.2694 - mae: 0.3672\nEpoch 97: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n212/212 [==============================] - 22s 105ms/step - loss: 0.2698 - mae: 0.3673 - val_loss: 22.4248 - val_mae: 2.5674 - lr: 2.2877e-04\nEpoch 98/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2547 - mae: 0.3456 - val_loss: 60.3518 - val_mae: 6.3304 - lr: 2.0589e-04\nEpoch 99/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.2427 - mae: 0.3389 - val_loss: 18.5167 - val_mae: 2.2239 - lr: 2.0589e-04\nEpoch 100/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2209 - mae: 0.3366 - val_loss: 11.6792 - val_mae: 1.5302 - lr: 2.0589e-04\nEpoch 101/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.2918 - mae: 0.3621 - val_loss: 66.4359 - val_mae: 6.5505 - lr: 2.0589e-04\nEpoch 102/250\n211/212 [============================>.] - ETA: 0s - loss: 0.3065 - mae: 0.3537\nEpoch 102: ReduceLROnPlateau reducing learning rate to 0.00018530203378759326.\n212/212 [==============================] - 22s 105ms/step - loss: 0.3067 - mae: 0.3538 - val_loss: 10.9632 - val_mae: 1.4178 - lr: 2.0589e-04\nEpoch 103/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.2368 - mae: 0.3353 - val_loss: 16.7256 - val_mae: 2.0282 - lr: 1.8530e-04\nEpoch 104/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.2899 - mae: 0.3498 - val_loss: 8.5260 - val_mae: 1.0244 - lr: 1.8530e-04\nEpoch 105/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.2337 - mae: 0.3407 - val_loss: 10.4771 - val_mae: 1.4189 - lr: 1.8530e-04\nEpoch 106/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.2725 - mae: 0.3595 - val_loss: 61.9758 - val_mae: 6.2657 - lr: 1.8530e-04\nEpoch 107/250\n212/212 [==============================] - ETA: 0s - loss: 0.2150 - mae: 0.3286\nEpoch 107: ReduceLROnPlateau reducing learning rate to 0.00016677183302817866.\n212/212 [==============================] - 23s 106ms/step - loss: 0.2150 - mae: 0.3286 - val_loss: 32.8525 - val_mae: 3.6295 - lr: 1.8530e-04\nEpoch 108/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.2588 - mae: 0.3550 - val_loss: 45.1259 - val_mae: 4.8914 - lr: 1.6677e-04\nEpoch 109/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.5064 - mae: 0.4202 - val_loss: 61.9653 - val_mae: 6.2313 - lr: 1.6677e-04\nEpoch 110/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.5660 - mae: 0.4183 - val_loss: 46.4231 - val_mae: 4.9478 - lr: 1.6677e-04\nEpoch 111/250\n212/212 [==============================] - 22s 104ms/step - loss: 0.3648 - mae: 0.3814 - val_loss: 10.4731 - val_mae: 1.3666 - lr: 1.6677e-04\nEpoch 112/250\n211/212 [============================>.] - ETA: 0s - loss: 0.2339 - mae: 0.3417\nEpoch 112: ReduceLROnPlateau reducing learning rate to 0.00015009464841568844.\n212/212 [==============================] - 22s 105ms/step - loss: 0.2339 - mae: 0.3417 - val_loss: 13.8235 - val_mae: 1.7459 - lr: 1.6677e-04\nEpoch 113/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2424 - mae: 0.3399 - val_loss: 11.1403 - val_mae: 1.4536 - lr: 1.5009e-04\nEpoch 114/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.2392 - mae: 0.3323 - val_loss: 9.4548 - val_mae: 1.2375 - lr: 1.5009e-04\nEpoch 115/250\n212/212 [==============================] - 23s 111ms/step - loss: 0.2023 - mae: 0.3243 - val_loss: 6.1201 - val_mae: 0.7814 - lr: 1.5009e-04\nEpoch 116/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2198 - mae: 0.3260 - val_loss: 19.1809 - val_mae: 2.3968 - lr: 1.5009e-04\nEpoch 117/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2800 - mae: 0.3590 - val_loss: 23.3847 - val_mae: 2.8173 - lr: 1.5009e-04\nEpoch 118/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.2122 - mae: 0.3291 - val_loss: 9.4149 - val_mae: 1.1410 - lr: 1.5009e-04\nEpoch 119/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2061 - mae: 0.3225 - val_loss: 11.1766 - val_mae: 1.4869 - lr: 1.5009e-04\nEpoch 120/250\n211/212 [============================>.] - ETA: 0s - loss: 0.2392 - mae: 0.3245\nEpoch 120: ReduceLROnPlateau reducing learning rate to 0.0001350851875031367.\n212/212 [==============================] - 23s 106ms/step - loss: 0.2434 - mae: 0.3250 - val_loss: 9.4425 - val_mae: 1.2356 - lr: 1.5009e-04\nEpoch 121/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.2318 - mae: 0.3359 - val_loss: 9.0741 - val_mae: 1.2191 - lr: 1.3509e-04\nEpoch 122/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.2091 - mae: 0.3207 - val_loss: 6.1600 - val_mae: 0.8295 - lr: 1.3509e-04\nEpoch 123/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2208 - mae: 0.3270 - val_loss: 13.2615 - val_mae: 1.5884 - lr: 1.3509e-04\nEpoch 124/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.2069 - mae: 0.3285 - val_loss: 22.1255 - val_mae: 2.3228 - lr: 1.3509e-04\nEpoch 125/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1890 - mae: 0.3190\nEpoch 125: ReduceLROnPlateau reducing learning rate to 0.00012157666351413355.\n212/212 [==============================] - 22s 106ms/step - loss: 0.1895 - mae: 0.3191 - val_loss: 6.6644 - val_mae: 0.9600 - lr: 1.3509e-04\nEpoch 126/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2147 - mae: 0.3227 - val_loss: 10.5381 - val_mae: 1.2855 - lr: 1.2158e-04\nEpoch 127/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.2282 - mae: 0.3321 - val_loss: 18.5442 - val_mae: 2.0481 - lr: 1.2158e-04\nEpoch 128/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1948 - mae: 0.3155 - val_loss: 41.2320 - val_mae: 4.2647 - lr: 1.2158e-04\nEpoch 129/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1833 - mae: 0.3113 - val_loss: 7.4570 - val_mae: 0.9718 - lr: 1.2158e-04\nEpoch 130/250\n211/212 [============================>.] - ETA: 0s - loss: 0.2193 - mae: 0.3205\nEpoch 130: ReduceLROnPlateau reducing learning rate to 0.00010941899454337544.\n212/212 [==============================] - 22s 105ms/step - loss: 0.2214 - mae: 0.3209 - val_loss: 48.7147 - val_mae: 5.0042 - lr: 1.2158e-04\nEpoch 131/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2272 - mae: 0.3237 - val_loss: 10.1444 - val_mae: 1.2477 - lr: 1.0942e-04\nEpoch 132/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1822 - mae: 0.3120 - val_loss: 9.1731 - val_mae: 1.1268 - lr: 1.0942e-04\nEpoch 133/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.2146 - mae: 0.3195 - val_loss: 13.7281 - val_mae: 1.5147 - lr: 1.0942e-04\nEpoch 134/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.1791 - mae: 0.3096 - val_loss: 8.4476 - val_mae: 1.0263 - lr: 1.0942e-04\nEpoch 135/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1803 - mae: 0.3065\nEpoch 135: ReduceLROnPlateau reducing learning rate to 9.847709443420172e-05.\n212/212 [==============================] - 22s 106ms/step - loss: 0.1948 - mae: 0.3075 - val_loss: 11.5160 - val_mae: 1.4032 - lr: 1.0942e-04\nEpoch 136/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2895 - mae: 0.3421 - val_loss: 20.2330 - val_mae: 2.3260 - lr: 9.8477e-05\nEpoch 137/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.2467 - mae: 0.3193 - val_loss: 11.5528 - val_mae: 1.4051 - lr: 9.8477e-05\nEpoch 138/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2619 - mae: 0.3327 - val_loss: 32.7519 - val_mae: 3.6161 - lr: 9.8477e-05\nEpoch 139/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.2339 - mae: 0.3377 - val_loss: 12.0793 - val_mae: 1.5403 - lr: 9.8477e-05\nEpoch 140/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1903 - mae: 0.3100\nEpoch 140: ReduceLROnPlateau reducing learning rate to 8.862938630045391e-05.\n212/212 [==============================] - 22s 105ms/step - loss: 0.1987 - mae: 0.3107 - val_loss: 6.8290 - val_mae: 0.8810 - lr: 9.8477e-05\nEpoch 141/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2240 - mae: 0.3250 - val_loss: 12.6609 - val_mae: 1.5782 - lr: 8.8629e-05\nEpoch 142/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.1794 - mae: 0.3066 - val_loss: 6.7395 - val_mae: 0.8301 - lr: 8.8629e-05\nEpoch 143/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.1937 - mae: 0.3053 - val_loss: 7.4707 - val_mae: 0.9752 - lr: 8.8629e-05\nEpoch 144/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.2017 - mae: 0.3164 - val_loss: 9.6784 - val_mae: 1.1581 - lr: 8.8629e-05\nEpoch 145/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1913 - mae: 0.3183\nEpoch 145: ReduceLROnPlateau reducing learning rate to 7.976644701557234e-05.\n212/212 [==============================] - 22s 106ms/step - loss: 0.2038 - mae: 0.3192 - val_loss: 12.6691 - val_mae: 1.5867 - lr: 8.8629e-05\nEpoch 146/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.2194 - mae: 0.3269 - val_loss: 9.3590 - val_mae: 1.1723 - lr: 7.9766e-05\nEpoch 147/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1793 - mae: 0.3080 - val_loss: 7.4635 - val_mae: 0.9376 - lr: 7.9766e-05\nEpoch 148/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1880 - mae: 0.3102 - val_loss: 8.2393 - val_mae: 1.0788 - lr: 7.9766e-05\nEpoch 149/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1907 - mae: 0.3073 - val_loss: 6.6410 - val_mae: 0.8187 - lr: 7.9766e-05\nEpoch 150/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1859 - mae: 0.3069\nEpoch 150: ReduceLROnPlateau reducing learning rate to 7.178980231401511e-05.\n212/212 [==============================] - 22s 106ms/step - loss: 0.1925 - mae: 0.3075 - val_loss: 9.6606 - val_mae: 1.1971 - lr: 7.9766e-05\nEpoch 151/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1829 - mae: 0.3112 - val_loss: 7.8416 - val_mae: 0.9672 - lr: 7.1790e-05\nEpoch 152/250\n212/212 [==============================] - 24s 111ms/step - loss: 0.1759 - mae: 0.3013 - val_loss: 6.3156 - val_mae: 0.7680 - lr: 7.1790e-05\nEpoch 153/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1755 - mae: 0.3022 - val_loss: 6.6429 - val_mae: 0.8405 - lr: 7.1790e-05\nEpoch 154/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1746 - mae: 0.3022 - val_loss: 6.3659 - val_mae: 0.7916 - lr: 7.1790e-05\nEpoch 155/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1803 - mae: 0.3012 - val_loss: 6.2347 - val_mae: 0.8076 - lr: 7.1790e-05\nEpoch 156/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1967 - mae: 0.3069 - val_loss: 6.9606 - val_mae: 0.9118 - lr: 7.1790e-05\nEpoch 157/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1645 - mae: 0.2953\nEpoch 157: ReduceLROnPlateau reducing learning rate to 6.461082011810504e-05.\n212/212 [==============================] - 23s 107ms/step - loss: 0.1677 - mae: 0.2958 - val_loss: 8.8242 - val_mae: 1.0915 - lr: 7.1790e-05\nEpoch 158/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1934 - mae: 0.3069 - val_loss: 10.7232 - val_mae: 1.2555 - lr: 6.4611e-05\nEpoch 159/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1838 - mae: 0.3030 - val_loss: 8.6312 - val_mae: 1.0425 - lr: 6.4611e-05\nEpoch 160/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1753 - mae: 0.3002 - val_loss: 32.1694 - val_mae: 3.4954 - lr: 6.4611e-05\nEpoch 161/250\n212/212 [==============================] - 23s 110ms/step - loss: 0.1798 - mae: 0.3011 - val_loss: 6.3073 - val_mae: 0.7653 - lr: 6.4611e-05\nEpoch 162/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1814 - mae: 0.2981 - val_loss: 13.0824 - val_mae: 1.6253 - lr: 6.4611e-05\nEpoch 163/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1688 - mae: 0.3016 - val_loss: 6.5628 - val_mae: 0.8191 - lr: 6.4611e-05\nEpoch 164/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1798 - mae: 0.2986 - val_loss: 7.7674 - val_mae: 1.0214 - lr: 6.4611e-05\nEpoch 165/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1690 - mae: 0.2984 - val_loss: 8.1184 - val_mae: 0.9915 - lr: 6.4611e-05\nEpoch 166/250\n212/212 [==============================] - ETA: 0s - loss: 0.1627 - mae: 0.2949\nEpoch 166: ReduceLROnPlateau reducing learning rate to 5.8149741380475466e-05.\n212/212 [==============================] - 23s 107ms/step - loss: 0.1627 - mae: 0.2949 - val_loss: 13.3151 - val_mae: 1.6749 - lr: 6.4611e-05\nEpoch 167/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1755 - mae: 0.2945 - val_loss: 8.0434 - val_mae: 1.0052 - lr: 5.8150e-05\nEpoch 168/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2209 - mae: 0.3080 - val_loss: 13.2162 - val_mae: 1.5702 - lr: 5.8150e-05\nEpoch 169/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1673 - mae: 0.2952 - val_loss: 6.3931 - val_mae: 0.7932 - lr: 5.8150e-05\nEpoch 170/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1630 - mae: 0.2887 - val_loss: 6.9787 - val_mae: 0.8693 - lr: 5.8150e-05\nEpoch 171/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1668 - mae: 0.2964\nEpoch 171: ReduceLROnPlateau reducing learning rate to 5.233476658759173e-05.\n212/212 [==============================] - 23s 107ms/step - loss: 0.1710 - mae: 0.2969 - val_loss: 7.6190 - val_mae: 1.0282 - lr: 5.8150e-05\nEpoch 172/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1768 - mae: 0.3031 - val_loss: 7.1485 - val_mae: 0.9053 - lr: 5.2335e-05\nEpoch 173/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1674 - mae: 0.2931 - val_loss: 7.4259 - val_mae: 0.9032 - lr: 5.2335e-05\nEpoch 174/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.1702 - mae: 0.2969 - val_loss: 6.4760 - val_mae: 0.8560 - lr: 5.2335e-05\nEpoch 175/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1740 - mae: 0.3020 - val_loss: 6.2855 - val_mae: 0.8390 - lr: 5.2335e-05\nEpoch 176/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1708 - mae: 0.2941\nEpoch 176: ReduceLROnPlateau reducing learning rate to 4.7101289601414466e-05.\n212/212 [==============================] - 23s 107ms/step - loss: 0.1708 - mae: 0.2942 - val_loss: 7.9041 - val_mae: 0.9978 - lr: 5.2335e-05\nEpoch 177/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1602 - mae: 0.2912 - val_loss: 8.4058 - val_mae: 1.0723 - lr: 4.7101e-05\nEpoch 178/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1644 - mae: 0.2951 - val_loss: 8.3612 - val_mae: 1.0518 - lr: 4.7101e-05\nEpoch 179/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1632 - mae: 0.2898 - val_loss: 6.1050 - val_mae: 0.7763 - lr: 4.7101e-05\nEpoch 180/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1616 - mae: 0.2921 - val_loss: 6.2022 - val_mae: 0.7922 - lr: 4.7101e-05\nEpoch 181/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1575 - mae: 0.2904\nEpoch 181: ReduceLROnPlateau reducing learning rate to 4.239116096869111e-05.\n212/212 [==============================] - 22s 105ms/step - loss: 0.1598 - mae: 0.2908 - val_loss: 6.5031 - val_mae: 0.8377 - lr: 4.7101e-05\nEpoch 182/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1596 - mae: 0.2906 - val_loss: 6.3330 - val_mae: 0.8060 - lr: 4.2391e-05\nEpoch 183/250\n212/212 [==============================] - 23s 109ms/step - loss: 0.1574 - mae: 0.2881 - val_loss: 6.1493 - val_mae: 0.7439 - lr: 4.2391e-05\nEpoch 184/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1589 - mae: 0.2890 - val_loss: 6.4215 - val_mae: 0.8309 - lr: 4.2391e-05\nEpoch 185/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1603 - mae: 0.2872 - val_loss: 7.5129 - val_mae: 0.9682 - lr: 4.2391e-05\nEpoch 186/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.2059 - mae: 0.2975 - val_loss: 9.5724 - val_mae: 1.1445 - lr: 4.2391e-05\nEpoch 187/250\n212/212 [==============================] - 24s 112ms/step - loss: 0.1567 - mae: 0.2868 - val_loss: 6.1079 - val_mae: 0.7423 - lr: 4.2391e-05\nEpoch 188/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1626 - mae: 0.2904 - val_loss: 7.6778 - val_mae: 0.9755 - lr: 4.2391e-05\nEpoch 189/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1679 - mae: 0.2916 - val_loss: 6.0501 - val_mae: 0.7974 - lr: 4.2391e-05\nEpoch 190/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1664 - mae: 0.2980 - val_loss: 6.8239 - val_mae: 0.8784 - lr: 4.2391e-05\nEpoch 191/250\n212/212 [==============================] - 23s 111ms/step - loss: 0.1581 - mae: 0.2879 - val_loss: 5.9409 - val_mae: 0.7354 - lr: 4.2391e-05\nEpoch 192/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1756 - mae: 0.2936 - val_loss: 6.0450 - val_mae: 0.7510 - lr: 4.2391e-05\nEpoch 193/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1765 - mae: 0.2964 - val_loss: 6.5145 - val_mae: 0.8531 - lr: 4.2391e-05\nEpoch 194/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1799 - mae: 0.2961 - val_loss: 6.5141 - val_mae: 0.8026 - lr: 4.2391e-05\nEpoch 195/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1642 - mae: 0.2881 - val_loss: 6.2660 - val_mae: 0.7716 - lr: 4.2391e-05\nEpoch 196/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1502 - mae: 0.2850\nEpoch 196: ReduceLROnPlateau reducing learning rate to 3.815204618149437e-05.\n212/212 [==============================] - 22s 106ms/step - loss: 0.1502 - mae: 0.2850 - val_loss: 6.1489 - val_mae: 0.7444 - lr: 4.2391e-05\nEpoch 197/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1693 - mae: 0.2857 - val_loss: 8.9605 - val_mae: 1.1334 - lr: 3.8152e-05\nEpoch 198/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1690 - mae: 0.2875 - val_loss: 6.0115 - val_mae: 0.7422 - lr: 3.8152e-05\nEpoch 199/250\n212/212 [==============================] - 23s 110ms/step - loss: 0.1582 - mae: 0.2878 - val_loss: 5.8784 - val_mae: 0.7305 - lr: 3.8152e-05\nEpoch 200/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1589 - mae: 0.2848 - val_loss: 6.1460 - val_mae: 0.7512 - lr: 3.8152e-05\nEpoch 201/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1594 - mae: 0.2900 - val_loss: 6.0817 - val_mae: 0.7443 - lr: 3.8152e-05\nEpoch 202/250\n212/212 [==============================] - 23s 109ms/step - loss: 0.1628 - mae: 0.2869 - val_loss: 5.7292 - val_mae: 0.7288 - lr: 3.8152e-05\nEpoch 203/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1613 - mae: 0.2879 - val_loss: 6.0930 - val_mae: 0.7578 - lr: 3.8152e-05\nEpoch 204/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1621 - mae: 0.2890 - val_loss: 5.9213 - val_mae: 0.7818 - lr: 3.8152e-05\nEpoch 205/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1577 - mae: 0.2912 - val_loss: 5.9811 - val_mae: 0.7476 - lr: 3.8152e-05\nEpoch 206/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1495 - mae: 0.2839 - val_loss: 5.7986 - val_mae: 0.7407 - lr: 3.8152e-05\nEpoch 207/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1565 - mae: 0.2869\nEpoch 207: ReduceLROnPlateau reducing learning rate to 3.4336842873017304e-05.\n212/212 [==============================] - 22s 105ms/step - loss: 0.1589 - mae: 0.2873 - val_loss: 6.1108 - val_mae: 0.8034 - lr: 3.8152e-05\nEpoch 208/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1495 - mae: 0.2867 - val_loss: 6.0443 - val_mae: 0.7501 - lr: 3.4337e-05\nEpoch 209/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1666 - mae: 0.2870 - val_loss: 7.3249 - val_mae: 0.8991 - lr: 3.4337e-05\nEpoch 210/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1573 - mae: 0.2839 - val_loss: 5.8174 - val_mae: 0.7482 - lr: 3.4337e-05\nEpoch 211/250\n212/212 [==============================] - 23s 110ms/step - loss: 0.1594 - mae: 0.2867 - val_loss: 5.6516 - val_mae: 0.7158 - lr: 3.4337e-05\nEpoch 212/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1673 - mae: 0.2894 - val_loss: 7.1246 - val_mae: 0.9171 - lr: 3.4337e-05\nEpoch 213/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1856 - mae: 0.2902 - val_loss: 5.9475 - val_mae: 0.7315 - lr: 3.4337e-05\nEpoch 214/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1541 - mae: 0.2844 - val_loss: 6.0096 - val_mae: 0.7253 - lr: 3.4337e-05\nEpoch 215/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1766 - mae: 0.2851 - val_loss: 6.3649 - val_mae: 0.8417 - lr: 3.4337e-05\nEpoch 216/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1619 - mae: 0.2925\nEpoch 216: ReduceLROnPlateau reducing learning rate to 3.0903160222806036e-05.\n212/212 [==============================] - 22s 105ms/step - loss: 0.1620 - mae: 0.2926 - val_loss: 6.8605 - val_mae: 0.8704 - lr: 3.4337e-05\nEpoch 217/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1619 - mae: 0.2855 - val_loss: 5.9589 - val_mae: 0.7311 - lr: 3.0903e-05\nEpoch 218/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1472 - mae: 0.2813 - val_loss: 5.8563 - val_mae: 0.7204 - lr: 3.0903e-05\nEpoch 219/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1548 - mae: 0.2851 - val_loss: 5.9278 - val_mae: 0.7489 - lr: 3.0903e-05\nEpoch 220/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1575 - mae: 0.2829 - val_loss: 5.9880 - val_mae: 0.7742 - lr: 3.0903e-05\nEpoch 221/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1495 - mae: 0.2816\nEpoch 221: ReduceLROnPlateau reducing learning rate to 2.7812844200525434e-05.\n212/212 [==============================] - 22s 106ms/step - loss: 0.1523 - mae: 0.2821 - val_loss: 6.0813 - val_mae: 0.7517 - lr: 3.0903e-05\nEpoch 222/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1575 - mae: 0.2833 - val_loss: 5.9943 - val_mae: 0.7322 - lr: 2.7813e-05\nEpoch 223/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.1726 - mae: 0.2848 - val_loss: 5.9647 - val_mae: 0.7204 - lr: 2.7813e-05\nEpoch 224/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1750 - mae: 0.2819 - val_loss: 13.1849 - val_mae: 1.6377 - lr: 2.7813e-05\nEpoch 225/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1489 - mae: 0.2833 - val_loss: 5.9704 - val_mae: 0.7425 - lr: 2.7813e-05\nEpoch 226/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1484 - mae: 0.2824\nEpoch 226: ReduceLROnPlateau reducing learning rate to 2.5031560107890984e-05.\n212/212 [==============================] - 23s 106ms/step - loss: 0.1499 - mae: 0.2827 - val_loss: 5.6978 - val_mae: 0.7171 - lr: 2.7813e-05\nEpoch 227/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1567 - mae: 0.2855 - val_loss: 5.8537 - val_mae: 0.7191 - lr: 2.5032e-05\nEpoch 228/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1561 - mae: 0.2818 - val_loss: 5.8352 - val_mae: 0.7175 - lr: 2.5032e-05\nEpoch 229/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1504 - mae: 0.2830 - val_loss: 6.0504 - val_mae: 0.7590 - lr: 2.5032e-05\nEpoch 230/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.1530 - mae: 0.2799 - val_loss: 6.8337 - val_mae: 0.8687 - lr: 2.5032e-05\nEpoch 231/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1447 - mae: 0.2814\nEpoch 231: ReduceLROnPlateau reducing learning rate to 2.2528404588229024e-05.\n212/212 [==============================] - 23s 107ms/step - loss: 0.1453 - mae: 0.2816 - val_loss: 6.2919 - val_mae: 0.7817 - lr: 2.5032e-05\nEpoch 232/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1487 - mae: 0.2800 - val_loss: 5.9141 - val_mae: 0.7407 - lr: 2.2528e-05\nEpoch 233/250\n212/212 [==============================] - 23s 106ms/step - loss: 0.1575 - mae: 0.2818 - val_loss: 5.8254 - val_mae: 0.7361 - lr: 2.2528e-05\nEpoch 234/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1530 - mae: 0.2848 - val_loss: 5.9346 - val_mae: 0.7541 - lr: 2.2528e-05\nEpoch 235/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1512 - mae: 0.2811 - val_loss: 5.7649 - val_mae: 0.7188 - lr: 2.2528e-05\nEpoch 236/250\n212/212 [==============================] - ETA: 0s - loss: 0.1586 - mae: 0.2836\nEpoch 236: ReduceLROnPlateau reducing learning rate to 2.0275563474569936e-05.\n212/212 [==============================] - 22s 105ms/step - loss: 0.1586 - mae: 0.2836 - val_loss: 6.9469 - val_mae: 0.8352 - lr: 2.2528e-05\nEpoch 237/250\n212/212 [==============================] - 23s 110ms/step - loss: 0.1406 - mae: 0.2766 - val_loss: 5.5682 - val_mae: 0.7036 - lr: 2.0276e-05\nEpoch 238/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1503 - mae: 0.2804 - val_loss: 5.8423 - val_mae: 0.7741 - lr: 2.0276e-05\nEpoch 239/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1576 - mae: 0.2820 - val_loss: 6.0726 - val_mae: 0.7885 - lr: 2.0276e-05\nEpoch 240/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1571 - mae: 0.2829 - val_loss: 6.1007 - val_mae: 0.7412 - lr: 2.0276e-05\nEpoch 241/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1455 - mae: 0.2788 - val_loss: 6.0227 - val_mae: 0.7497 - lr: 2.0276e-05\nEpoch 242/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1459 - mae: 0.2783\nEpoch 242: ReduceLROnPlateau reducing learning rate to 1.8248007290821987e-05.\n212/212 [==============================] - 23s 107ms/step - loss: 0.1464 - mae: 0.2784 - val_loss: 5.8162 - val_mae: 0.7354 - lr: 2.0276e-05\nEpoch 243/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1565 - mae: 0.2793 - val_loss: 5.8392 - val_mae: 0.7229 - lr: 1.8248e-05\nEpoch 244/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1482 - mae: 0.2773 - val_loss: 6.0506 - val_mae: 0.7450 - lr: 1.8248e-05\nEpoch 245/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1455 - mae: 0.2797 - val_loss: 6.0361 - val_mae: 0.7486 - lr: 1.8248e-05\nEpoch 246/250\n212/212 [==============================] - 23s 107ms/step - loss: 0.1493 - mae: 0.2774 - val_loss: 5.7597 - val_mae: 0.7215 - lr: 1.8248e-05\nEpoch 247/250\n211/212 [============================>.] - ETA: 0s - loss: 0.1464 - mae: 0.2804\nEpoch 247: ReduceLROnPlateau reducing learning rate to 1.6423206398030745e-05.\n212/212 [==============================] - 23s 106ms/step - loss: 0.1506 - mae: 0.2809 - val_loss: 5.7782 - val_mae: 0.7116 - lr: 1.8248e-05\nEpoch 248/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1395 - mae: 0.2754 - val_loss: 5.7889 - val_mae: 0.7220 - lr: 1.6423e-05\nEpoch 249/250\n212/212 [==============================] - 22s 106ms/step - loss: 0.1398 - mae: 0.2762 - val_loss: 5.7753 - val_mae: 0.7319 - lr: 1.6423e-05\nEpoch 250/250\n212/212 [==============================] - 22s 105ms/step - loss: 0.1490 - mae: 0.2794 - val_loss: 5.7819 - val_mae: 0.7165 - lr: 1.6423e-05\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_weights(ckp_path)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T06:39:45.720478Z","iopub.execute_input":"2023-04-15T06:39:45.722057Z","iopub.status.idle":"2023-04-15T06:39:47.539992Z","shell.execute_reply.started":"2023-04-15T06:39:45.722007Z","shell.execute_reply":"2023-04-15T06:39:47.538888Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x726af56e2f90>"},"metadata":{}}]},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the model.\nwith open('model.tflite', 'wb') as f:\n  f.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T06:39:47.541727Z","iopub.execute_input":"2023-04-15T06:39:47.542076Z","iopub.status.idle":"2023-04-15T06:41:05.707421Z","shell.execute_reply.started":"2023-04-15T06:39:47.542046Z","shell.execute_reply":"2023-04-15T06:41:05.706319Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"prediction_val=model.predict(X_test,batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T06:41:05.709358Z","iopub.execute_input":"2023-04-15T06:41:05.709758Z","iopub.status.idle":"2023-04-15T06:41:26.489932Z","shell.execute_reply.started":"2023-04-15T06:41:05.709718Z","shell.execute_reply":"2023-04-15T06:41:26.488777Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"38/38 [==============================] - 3s 27ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"prediction_val[:20]","metadata":{"execution":{"iopub.status.busy":"2023-04-15T07:00:49.328720Z","iopub.execute_input":"2023-04-15T07:00:49.329500Z","iopub.status.idle":"2023-04-15T07:00:49.337362Z","shell.execute_reply.started":"2023-04-15T07:00:49.329465Z","shell.execute_reply":"2023-04-15T07:00:49.335887Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"array([[1.29363089e+01],\n       [5.00449467e+00],\n       [4.05181313e+00],\n       [1.39345264e+01],\n       [1.42125034e+01],\n       [2.65003276e+00],\n       [1.20614977e+01],\n       [1.80131721e+01],\n       [1.97481995e+01],\n       [1.58019028e+01],\n       [5.60304356e+00],\n       [1.18319016e+01],\n       [8.91456413e+00],\n       [1.34355850e+01],\n       [2.29922943e+01],\n       [1.31233015e+01],\n       [1.80397339e+01],\n       [2.06177101e+01],\n       [1.86725855e-02],\n       [2.56661568e+01]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}